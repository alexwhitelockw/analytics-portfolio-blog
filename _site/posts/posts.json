[
  {
    "path": "posts/2022-02-03-tidytuesday-chocolate-ratings/",
    "title": "TidyTuesday Chocolate Ratings",
    "description": "Using tidymodels to predict chocolate ratings based on the memorable characteristics.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2022-02-03",
    "categories": [],
    "contents": "\nOverview\nThis is my first attempt at using the tidymodels package to build a predictive model. For this project, I use the tidytuesdayR (Hughes 2022) chocolate rating data from 18-01-2022. I used the following resources to help me with this project:\nJulia Silge’s blog post on the same data\nThe Tidy Modeling with R book\nThe Supervised Machine Learning for Text Analysis in R book\n\n\n# Libraries ---------------------------\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(textrecipes)\nlibrary(tidytext)\nlibrary(tidytuesdayR)\nlibrary(tidymodels)\n\n\n\n\n\n# Read Data ---------------------------\n\nchocolate_ratings <- \n  tt_load(\"2022-01-18\")\n\n\n\n    Downloading file 1 of 1: `chocolate.csv`\n\nchocolate_ratings <- \n  as.data.table(\n    chocolate_ratings$chocolate\n  )\n\n\n\nSetting Up the Model\nThe first step is to create the train/test split. We then pass the training set to the cross-validation sampling function; I have set the number of folds to 10.\nWe then have the model engine. As outcome variable (rating) is on a scale of 1 to 4, a linear regression is used. Although more sophisticated models are available, I stick with linear regression given its my first go in using tidymodels!\nAs above, the model will be the prediction of chocolate rating based on the memorable characteristics. The latter are the words used to describe the chocolate. We would expect the words to describe the chocolate to be related to the numeric rating. We wouldn’t expect chocolate described as chalky or acidic to receive a high rating – I could be wrong! Following the recipe specification, the descriptions are tokenized, filtered, and then re-weighted using tf-idf (term frequency inverse document frequency). It is important to note that we passed the tune function to the max number of tokens argument. This is a parameter we tune using the grid that follows – 10 values between 100 and 200 are passed to the max tokens argument with each run.\nFinally, we run the model using the resampled data, the linear model engine, and parameter grid.\n\n\nset.seed(03022022)\n\nchocolate_ratings_split <- initial_split(chocolate_ratings)  # Create train\n# and test data sets\n\ntrain_ratings <- training(chocolate_ratings_split)  # Extract training data\n\ntrain_cv_data <-  # Create 10 cross-validation splits\n  vfold_cv(\n    data = train_ratings,\n    v = 10\n    )\n\nrating_lm_mod <-  # Setup a linear regression engine\n  linear_reg() %>% \n  set_engine(\"lm\")\n\nrating_recipe <-  # Recipe is to predict rating based on characteristics of the product\n  recipe(rating ~ most_memorable_characteristics, data = train_ratings) %>%\n  step_tokenize(most_memorable_characteristics) %>%  \n  step_tokenfilter(most_memorable_characteristics, max_tokens = tune()) %>%  \n  step_tfidf(most_memorable_characteristics)\n# The tune function is passed to the token filter function - this is going to passed\n# to the grid search.\n\nrating_wf <-  # Create modelling workflow, contains: model engine and recipe\n  workflow() %>% \n  add_model(rating_lm_mod) %>% \n  add_recipe(rating_recipe)\n\nmax_token_grid <-  # Create grid for max tokens ranging from 100 to 200; 10 \n  grid_regular(  # different values are given\n    max_tokens(range = c(1e2, 2e2)),\n    levels = 10\n    )\n\nrating_lm_output <-\n  tune_grid(\n    rating_wf, \n    resamples = train_cv_data,\n    grid = max_token_grid,\n    metrics = metric_set(rmse, mae, mape),\n    control = control_resamples(save_pred = T)\n    )\n\n\n\nSelecting the Best Model\nNext, we collect the metrics from the linear regression models. Put differently, the metric (e.g., mean absolute error) values across each fold and max token value are averaged; Figure 1 presents these values. We can see that models performed better with a larger number of tokens, up until 200 tokens.\n\n\nrating_lm_output %>% \n  collect_metrics() %>%  # Mean metric value for each unique metric and max token value\n  ggplot(aes(x = max_tokens, y = mean, colour = .metric)) +\n  geom_point(alpha = .3, size = 2) +\n  geom_line(alpha = .3, size = 1.5) +\n  facet_wrap(~.metric, scales = \"free_y\", ncol = 1) +\n  labs(\n    x = \"Max Tokens\",\n    y = \"Mean\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nFigure 1: Average metrics (MSE, MAPE, and RMSE) across 10 max token values.\n\n\n\nThe best performing model can be selected as followed – the mean absolute error is used for this selection. This model is then used to create a finalised workflow and fitted to the training and testing data.\n\n\nlm_best_mae <-  # Select the best model based on the mean absolute error\n  rating_lm_output %>% \n  select_best(metric = \"mae\")\n\nfinalised_lm_wf <-  # Finalise the workflow associated with the best MAE\n  rating_wf %>% \n  finalize_workflow(lm_best_mae)\n\nfinal_model_fit <-  # Run the model and workflow on both the training and testing data\n  last_fit(finalised_lm_wf, \n           chocolate_ratings_split,\n           metrics = metric_set(mae, rmse, rsq))\n\n\n\nEvaluating the Final Model\nMetrics for the final model are presented below. The model accounts for ~40% of the variance in chocolate ratings; however, the mean absolute error is large given the rating scale of 1-4.\n\n\ncollect_metrics(final_model_fit) %>% \n  kbl(\n    booktabs = T,\n    col.names = c(\"Metric\", \"Estimator\", \"Estimate\", \"Model\"),\n    digits = 2,\n  ) %>% \n  kable_material()\n\n\n\nMetric\n\n\nEstimator\n\n\nEstimate\n\n\nModel\n\n\nmae\n\n\nstandard\n\n\n0.27\n\n\nPreprocessor1_Model1\n\n\nrmse\n\n\nstandard\n\n\n0.35\n\n\nPreprocessor1_Model1\n\n\nrsq\n\n\nstandard\n\n\n0.40\n\n\nPreprocessor1_Model1\n\n\nFigure 2 plots the actual values against the predicted values in the test data. It is clear that the model does not perform well. For instances where the actual rating is low (~1), the model tends to over-predict ratings. The model also tends to under-predict ratings for chocolate that received high ratings.\n\n\ncollect_predictions(final_model_fit) %>% \n  ggplot(aes(x = rating, y = .pred, colour = id)) +\n  geom_abline(color = \"red\") + \n  geom_jitter(\n    alpha = .3\n  ) +\n  scale_colour_viridis_d(option = \"plasma\") +\n  theme_bw() +\n  theme(\n    legend.position = \"none\"\n  ) +\n  coord_obs_pred() +\n  labs(\n    x = \"Rating\",\n    y = \"Prediction\",\n    colour = \"Fold\"\n  )\n\n\n\n\nFigure 2: Actual and predicted values. Large residuals are noted for chocolates that receive a rating below 3. In these cases, the model over-predicts the chocolate rating. At higher ratings, the model tends to underpredict the rating value.\n\n\n\nWe can go a step further and explore 10 instances with the largest absolute differences between actual and predicted values. Each instance is a chocolate that was not well rated; however, the predicted ratings were substantially higher. With characteristics such as strong off flavor, potting soil, and very bitter, it’s concerning that the model predicts a high score for these chocolates.\n\n\nfinal_model_residuals <-\n  collect_predictions(final_model_fit) %>% \n  mutate(\n    absolute_residual = abs(rating - .pred)\n  ) %>% \n  slice_max(order_by = absolute_residual, n = 10)\n\n\nchocolate_ratings %>% \n  slice(final_model_residuals$.row) %>% \n  bind_cols(final_model_residuals %>% select(.pred)) %>% \n  select(ref, most_memorable_characteristics, rating, .pred) %>% \n  kbl(\n    booktabs = T,\n    digits = 2,\n    col.names = c(\"Row Number\", \"Memorable Characteristics\", \"Rating\", \"Prediction\")) %>% \n  kable_material()\n\n\n\nRow Number\n\n\nMemorable Characteristics\n\n\nRating\n\n\nPrediction\n\n\n135\n\n\npastey, strong off flavor\n\n\n1.00\n\n\n2.69\n\n\n105\n\n\nmalitol, cocoa\n\n\n2.00\n\n\n3.58\n\n\n141\n\n\nsandy, dairy, cocoa\n\n\n2.00\n\n\n3.34\n\n\n363\n\n\ncocoa, dominate off note\n\n\n1.75\n\n\n3.02\n\n\n296\n\n\ncharred, espresso\n\n\n2.50\n\n\n3.75\n\n\n2194\n\n\npotting soil\n\n\n2.50\n\n\n3.75\n\n\n1271\n\n\nsticky, mild citrus, earthy\n\n\n2.25\n\n\n3.40\n\n\n252\n\n\nchalky, musty, very bitter\n\n\n1.00\n\n\n2.10\n\n\n693\n\n\nperfume, strong chemical\n\n\n1.50\n\n\n2.48\n\n\n615\n\n\nsticky, intense, very bitter\n\n\n1.50\n\n\n2.44\n\n\nWe can predict the rating of new data as follows.\n\n\npredict(\n  final_model_fit$.workflow[[1]],  # Pass the finalised workflow\n  data.frame(\n    most_memorable_characteristics = \"acidic, chemical\")  # Pass a dataframe containing\n  ) %>%  # the column used to train the model.\n  kbl(\n    booktabs = T,\n    col.names = \"Prediction\",\n    digits = 2\n  ) %>% \n  kable_material()\n\n\n\nPrediction\n\n\n3.53\n\n\nFeature Importance\nAs a last step, we can look at the importance of features in the model. To do this, we look at the top 10 point estimates based on whether the value exceeds 0 (Figure 3. There is a lot of uncertainty in the point estimates above 0 (e.g. tropical); this is not the case for point estimates below 0. We can interpret the point estimate as follows. For a chocolate described as bitter, we expect the rating to be .67 lower on average.\n\n\nextract_workflow(final_model_fit) %>% \n  tidy() %>% \n  filter(!grepl(\"Intercept\", term)) %>% \n  group_by(estimate > 0) %>% \n  slice_max(abs(estimate), n = 10) %>% \n  ungroup() %>% \n  mutate(\n    term = stringr::str_remove(term, \"tfidf_most_memorable_characteristics_\")\n  ) %>% \n  ggplot(aes(x = estimate, y = forcats::fct_reorder(term, estimate),\n             colour = estimate > 0)) +\n  geom_vline(\n    aes(xintercept = 0)\n  ) +\n  geom_point() +\n  geom_errorbarh(\n    aes(\n      xmin = estimate - std.error,\n      xmax = estimate + std.error\n    )\n  ) +\n  scale_colour_viridis_d(option = \"E\", direction = -1) +\n  theme_bw() +\n  labs(\n    x = \"Estimate\",\n    y = NULL\n  )\n\n\n\n\nFigure 3: Point estimates obtained from the final model. Most memorable characteristics with positive point estimates have a large amount of uncertainty. Negative point estimates have smaller standard errors.\n\n\n\n\n\n\nHughes, Ellis. 2022. tidytuesdayR: Access the Weekly ’TidyTuesday’ Project Dataset. https://CRAN.R-project.org/package=tidytuesdayR.\n\n\n\n\n",
    "preview": "posts/2022-02-03-tidytuesday-chocolate-ratings/tidytuesday-chocolate-ratings_files/figure-html5/lmmetricplot-1.png",
    "last_modified": "2022-02-11T19:02:22+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-29-using-the-rle-function-to-explore-t20-cricket-runs/",
    "title": "Computing Run Lengths of Runs in BBL",
    "description": "A demonstration of the rle function in R using BBL data.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2022-01-29",
    "categories": [
      "cricket",
      "rle"
    ],
    "contents": "\nBackground\nRun length encoding is a form of data compression, wherein a sequence is reduced to a value-count pair. It is considered lossless as the original information contained in the sequence is retained. For example, the sequence 001022 would be reduced to 0(2), 1(1), 0(1), and 2(2) value-count pairs. In R, this operation is carried out using the rle base function.\nFirst, we setup a vector of 10 random numbers between 1 and 5.\n\n\nset.seed(31012022)\n\nexample_vector <- \n  sample(1:5, size = 10, replace = T)\n\nexample_vector\n\n\n [1] 4 4 3 1 1 5 5 3 4 1\n\nThen we pass the vector to the rle function.\n\n\nexample_rle <-\n  rle(example_vector)\n\n\n\nThe example_rle is a list containing two elements: lengths and values. Values correspond to those values in the original example_vector; whereas, lengths indicates the run count for the given value.\nTo access a value, we can run the following\n\n\nexample_rle$values[4]  # Access the fourth value in the values vector\n\n\n\nThe length is accessed using\n\n\nexample_rle$lengths[4]  # Access the fourth value in the lengths vector\n\n\n\nWhat we learn is that the value 5 has a run length of 2 in the original vector.\nApplied Example\nA potential use case for the rle function is to explore the run lengths of cricket runs. For this, we will use the BBL (Big Bash League) data from Cricsheet.\nThe below code reads the scores for each match within the BBL (excluding the BBL11 final). We then split the data by match and batting team as we want to explore run lengths for each team by match. As implied, we’re looking at the whole innings as opposed to splitting the data by overs. Finally, for each match and batting team we look at the values of the max run lengths for each team; multiple values per match and batting team are a possibility. With this data, we’ll look at the count of most frequent run lengths for each team.\n\n\nfile_path_list <- \n  list.files(\"bbl_csv2/\", full.names = T)  \n\nfile_list <- lapply(file_path_list, function(file_path) {  # For each file path in list\n  if(grepl(\"[0-9]+.csv\", file_path)) {  # read in if they follow a particular pattern.\n    read.csv(file_path)  # Some file paths have 'info' in the name - this removes such data.\n  }\n})\n\nbbl_match_details <- \n  do.call(rbind, file_list)\n\nmatch_team_split <- \n  split(bbl_match_details,  # Split data by batting team and match.\n        f = ~ match_id + batting_team)\n\nrun_rle <- lapply(match_team_split, function(df) {\n  rle(df$runs_off_bat)  # Apply rle function to each dataset.\n})\n\n\nbbl_rle <- lapply(run_rle, function(df) {\n  index_max <- which(df$lengths == max(df$lengths))  # Identify indices where the value equals the max.\n  rle_df <- data.frame(\n    run_value = df$values[index_max],  # Create a dataframe based on above indices.\n    count_value = df$lengths[index_max]\n  )\n  return(rle_df)\n})\n\nbbl_rle <-\n  do.call(rbind, bbl_rle)\n\nbbl_rle$match_id <-\n  stringr::str_extract(row.names(bbl_rle), pattern = \"[0-9]+\")  # Extract match id\n\nbbl_rle$batting_team <-\n  stringr::str_extract(row.names(bbl_rle), pattern = \"[A-Za-z ]+\")  # Extract batting team name\n\nrow.names(bbl_rle) <- NULL\n\n\n\nThe final data is outputted below, ranked (descending) by run values. We can see that only Brisbane Heat had a run value of 6 in relation to the maximum run lengths in a given match; this occurred twice over the course of the BBL.\n\n\nrmarkdown::paged_table(\n  bbl_rle[order(-bbl_rle$run_value),] \n)\n\n\n\n\n\nAs a final step, we can count the number of times a given run value was found to be the maximum run length within a match. We do this with the tapply function, giving run_values as the vector, batting_team as the index, and table as the function. We can see that the Strikers, Renegades, and Scorchers have only run values of 0 and 1 as their most frequent run lengths.\n\n\ntapply(bbl_rle$run_value, bbl_rle$batting_team, table)\n\n\n$`Adelaide Strikers`\n\n  0   1 \n100 107 \n\n$`Brisbane Heat`\n\n 0  1  4  6 \n93 72  2  2 \n\n$`Hobart Hurricanes`\n\n 0  1  2  4 \n81 94  1  1 \n\n$`Melbourne Renegades`\n\n 0  1 \n90 67 \n\n$`Melbourne Stars`\n\n 0  1  4 \n92 97  2 \n\n$`Perth Scorchers`\n\n 0  1 \n89 85 \n\n$`Sydney Sixers`\n\n 0  1  2  4 \n91 80  1  1 \n\n$`Sydney Thunder`\n\n 0  1  2  4 \n97 79  1  3 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-01T20:35:55+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-17-victorian-alcohol-licences/",
    "title": "Victorian Alcohol Licences: A Correspondence Analysis",
    "description": "I look to use correspondence analysis to analyse Victorian alcohol licence categories.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-07-17",
    "categories": [
      "correspondence analysis",
      "victoria data"
    ],
    "contents": "\nThe general aim of this post was for me to learn how to use correspondence analysis. Data came from Data.Vic. This post and this article influenced the steps I took to analyse the data.\nLicence Category by Region\nLet’s start by creating a contingency table of licence category by region (Table 1). The row numbers are useful for interpreting the bi-plot that follows. The frequencies in the table show the Restaurant and Cafe Licence category to dominate across Metro areas, whilst the General Licence category does not dominate a region.\n\n\nTable 1: Licence category counts across 10 regions in Victoria.\n\n\n\n\nRegion\n\n\nBYO Permit\n\n\nFull Club Licence\n\n\nGeneral Licence\n\n\nLate night (general) Licence\n\n\nLate night (on-premises) Licence\n\n\nLate night (packaged liquor) Licence\n\n\nLimited Licence\n\n\nOn-Premises Licence\n\n\nPackaged Liquor Licence\n\n\nPre-retail Licence\n\n\nProducer’s Licence\n\n\nRestaurant and cafe Licence\n\n\nRestricted Club Licence\n\n\n1\n\n\nBarwon South-West\n\n\n61\n\n\n92\n\n\n182\n\n\n30\n\n\n7\n\n\n1\n\n\n522\n\n\n159\n\n\n164\n\n\n4\n\n\n84\n\n\n426\n\n\n91\n\n\n2\n\n\nGippsland\n\n\n62\n\n\n80\n\n\n149\n\n\n16\n\n\n11\n\n\n0\n\n\n365\n\n\n86\n\n\n138\n\n\n4\n\n\n66\n\n\n209\n\n\n64\n\n\n3\n\n\nGrampians\n\n\n26\n\n\n59\n\n\n188\n\n\n14\n\n\n4\n\n\n0\n\n\n318\n\n\n74\n\n\n94\n\n\n5\n\n\n111\n\n\n172\n\n\n65\n\n\n4\n\n\nHume\n\n\n109\n\n\n46\n\n\n259\n\n\n18\n\n\n9\n\n\n0\n\n\n481\n\n\n105\n\n\n126\n\n\n4\n\n\n185\n\n\n229\n\n\n88\n\n\n5\n\n\nLoddon Mallee\n\n\n40\n\n\n83\n\n\n235\n\n\n17\n\n\n10\n\n\n0\n\n\n481\n\n\n95\n\n\n135\n\n\n7\n\n\n173\n\n\n213\n\n\n88\n\n\n6\n\n\nMetro East\n\n\n247\n\n\n72\n\n\n102\n\n\n45\n\n\n19\n\n\n0\n\n\n830\n\n\n229\n\n\n322\n\n\n24\n\n\n114\n\n\n1024\n\n\n104\n\n\n7\n\n\nMetro North\n\n\n153\n\n\n89\n\n\n364\n\n\n195\n\n\n191\n\n\n2\n\n\n1151\n\n\n662\n\n\n439\n\n\n34\n\n\n60\n\n\n2021\n\n\n70\n\n\n8\n\n\nMetro South\n\n\n236\n\n\n135\n\n\n212\n\n\n97\n\n\n80\n\n\n0\n\n\n1334\n\n\n454\n\n\n498\n\n\n42\n\n\n142\n\n\n1545\n\n\n179\n\n\n9\n\n\nMetro West\n\n\n76\n\n\n71\n\n\n70\n\n\n38\n\n\n11\n\n\n0\n\n\n399\n\n\n141\n\n\n239\n\n\n11\n\n\n16\n\n\n591\n\n\n66\n\n\n10\n\n\nNorth West\n\n\n6\n\n\n5\n\n\n4\n\n\n4\n\n\n2\n\n\n0\n\n\n53\n\n\n10\n\n\n31\n\n\n1\n\n\n5\n\n\n51\n\n\n7\n\n\nCorrespondence Analysis\nOur next step is to use correspondence analysis. For this post, only two dimensions are extracted. The total inertia explained (90.6%) shows this to be a very good solution.\n\n\n\nFigure 1: Row and column profiles for based on the correspondence analysis of the licence category contingency table\n\n\n\nFigure 1 plots the co-ordinates for both the rows and columns. From this, we can meaningfully compare row items to one another; this also applies to column items. However, comparisons between row and column items are not appropriate. In order to make such interpretations, we start by looking at the contribution each element makes. We can do this from the perspective of rows (the regions) or columns (the licence category). For this work, it’s more appropriate to interpret regions in relation to the licence category.\nColumn Contribution\n\n\n\nFigure 2: Contibution of each licence category to the total interia explained by each dimension. The red line indicates the expected average value. Thus, any value exceeding this line would be considered significant.\n\n\n\nFigure 2 shows the contribution each licence category makes to dimensions 1 and 2.\nDimension 1\nFor dimension 1, three licence categories exceed the expected average value (7.69%): Producer’s Licence, Restaurant and Cafe Licence, and General Licence.\nThe Producer and General Licence categories have positive coordinates (Table 2) for dimension 1, whilst Restaurant and Cafe Licence has a negative coordinate. Thus, we can interpret this dimension as Business Type. On the positive-end, we have licence categories for producing alcohol and supplying liquor (on and off the premise). The negative-end is the licence category for providing alcohol in hospitality setting (restaurant or cafe). Thus, we have a dimension ranging from an alcohol licence for on-site consumption to licences permitting the production and take-away of alcohol.\nDimension 2\nFor dimension 2, three licence categories exceed the expected average value (7.69%): General Licence, BYO Permit, and Late Night (On-Premises) Licence.\nGeneral Licence and Late Night (On-Premises) Licence categories have positive coordinates for dimension 2. The BYO Permit licence category has a negative coordinate for dimension 2. This distinction suggests a dimension of Supply Type. One end is strictly purchasing alcohol on site, whilst the other allows customers to bring their own alcohol.\n\n\nTable 2: Licence category coordinates for dimensions 1 and 2.\n\n\n\n\nDim 1\n\n\nDim 2\n\n\nBYO Permit\n\n\n0.01\n\n\n-0.32\n\n\nFull Club Licence\n\n\n0.33\n\n\n-0.07\n\n\nGeneral Licence\n\n\n0.51\n\n\n0.27\n\n\nLate night (general) Licence\n\n\n-0.35\n\n\n0.26\n\n\nLate night (on-premises) Licence\n\n\n-0.55\n\n\n0.54\n\n\nLate night (packaged liquor) Licence\n\n\n-0.51\n\n\n0.91\n\n\nLimited Licence\n\n\n0.10\n\n\n-0.05\n\n\nOn-Premises Licence\n\n\n-0.19\n\n\n0.14\n\n\nPackaged Liquor Licence\n\n\n-0.04\n\n\n-0.13\n\n\nPre-retail Licence\n\n\n-0.25\n\n\n-0.12\n\n\nProducer’s Licence\n\n\n0.79\n\n\n0.04\n\n\nRestaurant and cafe Licence\n\n\n-0.30\n\n\n0.00\n\n\nRestricted Club Licence\n\n\n0.38\n\n\n-0.15\n\n\nRow Positions\nNow that we’ve done the above steps, we can readily interpret the region positions in relation to the licence categories (Figure 3; Table 1 translates the numeric value in the plot). For example, regions 3, 4, and 5 (Grampians, Hume, and Loddon Mallee) are wine regions. There would then be a greater requirement for a Producer’s Licence here.\n\n\n\nFigure 3: Row item positions relative to licence categories.\n\n\n\nOverview\nCorrespondence analysis of alcohol licence categories has offered an interesting, albeit obvious, insight. Metro area positions are relative to Restaurant and Cafe Licence, BYO Permit, and Late Night (On-Premises) Licence categories. Regional area positions are relative to the Producer’s Licence category. Despite this, it offered a useful way to analyse frequency data and will definitely use it again in the future!\n\n\n\n",
    "preview": "posts/2021-07-17-victorian-alcohol-licences/victorian-alcohol-licences_files/figure-html5/caBiPlot-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-23-victorian-premier-media-posts/",
    "title": "Victorian Premier Media Posts",
    "description": "I use structural topic modelling to explore the Victorian premier media posts over the course of 2019.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-07-16",
    "categories": [],
    "contents": "\nOverview\nUsing data extracted from here, we can explore how the Victorian Premier media posts change over the course of a year. Thus, I am trying to answer the question of how topic propensities in these media posts change with time? Dates of these posts spanned the time of 2016 to 2020. As I want to explore the change of topics over a year in recent posts, I only concentrate on 2019.\n\n\n\n\n\n\nModel Extraction\nFor brevity, I skip detail on pre-processing and identifying the best fitting model (7 topic for our purposes).\n\n\n\n\n\n\nFigure 1: Expected topic proportions for the seven topic model. Local government projects occur most frequently in the premier media posts, whilst the topic about health occur least frequently.\n\n\n\nFigure 1 presents the expected topic proportions for the 7 topic model using the 2019 data. I provide the top 5 words per topic to aid interpretation. We can see that posts cover a variety of topics: local projects, community support, sport, transport, education, and health.\nTopic Exploration\nLet’s explore the topics1!\n\n\n\nTopic One - Events\nBased on Figure 1, we may conclude that Topic 1 is sports-based. The associated quotes actually show us that the topic captures any events in Victoria: art exhibitions, crickets, and motor sport. The plot shows the propensity of the topic being mentioned in a media post to be consistent across 2019. There are peaks at the start and end of the year. This coincides with the timing of various sporting events (AFLW, NRL, BBL).\n\n\n\nTopic Five - Community Support\nOn words alone, Topic 5 captures media posts associated with supporting local communities and people (Figure 1). The examplar quotes support this conclusion. Quote 1 mentions the engagement of key stakeholders in advising the Victorian government on carer needs. Quote 2 is of a post mentioning new appointees to the Victorian Multicultural Commission. As a topic, the plot shows its propensity to be consistent across 2019.\n\n\n\nTopic Seven - Transportation\nFinally, we have Topic 7. Being a topic on transportation is apparent from the top topic words (Figure 1). The three topic quotes show media posts primarily addressing various works on transport routes (e.g., level crossing and intersections). Propensity to mention this topic in media posts shows peaks and troughs across 2019. These peaks are likely to be the announcement of construction blitzes prior to work beginning. Thus, troughs would coincide with the start of such works.\n\n\n\nSummary\nAfter previously using structural topic modelling to explore wine reviews, I was curious to explore how the propensity to mention topics changes with time. The Victorian Premier media posts offered a good dataset for this purpose. The results answered the question I posed. For some topics, they are consistent across the year. Whereas, others (e.g., transportation) follow a pattern.\n\nI choose only 3 topics so I don’t lapse into monotony↩︎\n",
    "preview": "posts/2021-04-23-victorian-premier-media-posts/victorian-premier-media-posts_files/figure-html5/topicProportions-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-cricket-performance/",
    "title": "Cricket Performance and Regression to the Mean",
    "description": "In this post I explore regression to the mean in the context of cricket performance.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-03-14",
    "categories": [
      "cricket",
      "regression"
    ],
    "contents": "\nOverview\nAfter watching the Australian Big Bash League (BBL), I’ve noticed that evaluations of player performances can be particularly critical. These critiques commonly directed towards a player whose performance is in a slump. Despite these negative appraisals, a player who may have experienced a bad season may improve in the season that follows. This is known as regression to the mean and is common to many walks of life, and is a result of imprecise measurements. In the context of cricket, a player’s performance is made up of both their true ability and random variation. Put differently, whilst a player has the skills to perform well, we cannot discount the role that luck plays. Therefore, predictions of future performances are unlikely to be perfect.\nIn this post, we will be exploring regression to the mean in the context of cricket player performance in the BBL (2011/12 Season to 2020/21 Season). All analyses undertaken used a dataset from Cricsheet.\nBatsman Performance\nLet’s start out with calculating the average runs per season.\n\n\n\n\nAs can be seen, there are 376 rows in the data with many blank values, as players have not played every season. For our purpose, we are going to look at those players who played three seasons (2018/19, 2019/20, and 2020/21). If any of these three seasons contained blank values, we dropped the player.\n\nTable 1: Average runs of those batsmen who played in the 2018/19, 2019/20, and 2020/21 BBL seasons (limited to top 10). Batsmen are in descending order based on the 2019/20 season. It can be seen that Wade attained the highest average run rate in the 2019/20 season, followed by Stoinis and Finch. In most cases, performances in the adjacent seasons were below that attained in the 2019/20 season.\nstriker\n2018/19\n2019/20\n2020/21\nMS Wade\n39.47\n43.88\n35.00\nMP Stoinis\n41.00\n41.47\n30.46\nAJ Finch\n27.50\n40.33\n13.77\nDJM Short\n42.47\n39.67\n24.64\nSE Marsh\n32.67\n37.42\n26.00\nAT Carey\n27.42\n35.55\n32.69\nJW Wells\n25.64\n34.14\n20.23\nTM Head\n21.00\n32.57\n17.50\nJR Philippe\n20.27\n30.44\n31.75\nBJ Webster\n15.75\n30.36\n16.00\n\nFrom Table 1, we can see those batsmen who performed well in the 2019/20 season. For most of these batsmen, performances in seasons adjacent to that attained in 2019/20 were not commensurate. For example, Webster scored an average of 30.36 in the 2019/20 season, whilst only averaged 15.75 and 16 runs in the 2018/2019 and 2020/21 seasons, respectively.\nWe can also look at the average run rate change across these top ten players:\nIn the 2018/19 season, the top ten players (Table 1) averaged 7.26 fewer runs compared to 2019/20.\nIn the 2020/21 season, the top ten players (Table 1) averaged 11.78 fewer runs compared to 2019/20.\nWe can take this analysis a step further by calculating correlations between the season performances. From this point onward, the analysis will use those 81 players without missing values across the three seasons (2018/19, 2019/20, and 2020/21).\nWhen we correlated the performance (average runs) of players in 2018/19, we get a value of 0.74. As for 2020/21, the correlation value is 0.68. In both cases, the correlations are positive and large, yet they are not perfect. We can say that past performance is a good indicator of future performance, yet predictions will not be 100% accurate.\nA simple linear model now follows, wherein we regress performance in the 2019/20 season on the prior season’s performance (2018/19; Table 2). Figure 1 plots the regression model of 2019/20 season performance on to 2018/19 season performance along with the regression line uncertainty.\n\n\nseason_1920_model <-\n  stan_glm(`2019/20` ~ `2018/19`,\n           data = top_strikers_2019,\n           refresh = 0)\n\nknitr::kable(\n  broom.mixed::tidy(season_1920_model),\n  digits = 2,\n  col.names = c(\"Term\", \"Estimate\", \"Standard Error\"),\n  caption = \"Point estimates from the regression of 2019/20 performance on 2018/19 performance.\")\n\n\nTable 2: Point estimates from the regression of 2019/20 performance on 2018/19 performance.\nTerm\nEstimate\nStandard Error\n(Intercept)\n3.61\n1.44\n2018/19\n0.78\n0.08\n\nBased on the model, if a player averaged 0 runs in the 2018/19 season, they are predicted to achieve an average run rate of 3.61 in the 2019/20 season.\nEach average run rate that is greater than 0 in the 2018/19 season results in an expected average run rate that is 0.78 higher than 3.61. As this coefficient is below 1, it is indicative of batting performance regressing to the mean. For example, if a player averaged 25 runs in 2018/19, they are predicted to average 22.87 runs in the 2019/20 season. In other words, player performance in one season is an imprecise measure of ability and we cannot discount the role that luck plays in any match.\nThe residual standard deviation of the model is 8. This means that our model predicts the average run rate in 2019/20 to within 8 runs. So while previous performance may be important in future performance, it isn’t enough to make a perfect prediction.\n\n\n\nFigure 1: Point estimate line for the regression of 2019/20 season performance onto the 2018/19 season performance. A sample of 500 coefficient values for the intercept and slope are also plotted to show the uncertainty in the estimates.\n\n\n\nSummary\nI’ve attempted to illustrate regression to the mean in cricket performances. As shown above, even though a player may perform well in a given year, this does not mean that future performances will be consistent. This is because performances are an imperfect measure of ability on account of random variations (i.e., a player may have had a lucky (or unlucky) season).\n\n\n\n",
    "preview": "posts/2021-03-06-cricket-performance/cricket_player_performance_files/figure-html5/plot1920Model-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-wine-reviews/",
    "title": "Wine Reviews",
    "description": "In this post I undertake a text-mining analysis of wine reviews obtained from Kaggle. I show that using both tf-idf and topic-modelling provides interesting insights into the words used to describe wines of different countries.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-02-23",
    "categories": [
      "wine",
      "text mining"
    ],
    "contents": "\n\n\n\n\n\n\nOverview\nThe following analysis uses the wine review data from Kaggle. It (the data) contains 14 columns, but we are only going to be concerned with three: country, the country from which the wine originated; description, the review of the wine; and variety, the grape type. With the removal of duplicates and rows containing missing values, the data contains 89123 rows.\nFor this post, the following steps will be taken in the analysis:\nAn exploration of keywords across a select number of countries;\nAn exploration of keywords across a select number of varieties;\nand, a topic modelling of reviews for wines originating from France, Italy, and Spain.\nKeyword Exploration\nKeywords will be identified using tf-idf, which involves weighting the frequency of a term within a document by the frequency of a word across documents. In other words, if a word is not found across many documents it can be thought of as important to the document in question.\nBelow, we explore tf-idf in relation to the variables of country and variety of wines. For each variable, we are only taking a subset of the data, especially when we have 46 and 619 unique countries and varieties, respectively.\nCountry Words\nFigure 1 presents the keywords associated with the wine of 11 different countries. For most cases, the top word is the main variety of wine for the country (e.g., Malbec is the leading grape in Argentina). Beyond the grape names, we can see that the remaining words are descriptors of the wine itself. Taking France as an example, we can see that the wine is described by the words fruits, wood, and character.\n\n\n\nFigure 1: Top keywords associated with the wines of 11 different countries based on tf-idf values. In majority of cases, the first word in each sub-plot is the wine variety for that country. Beyond this, it can be seen that different words are used to describe the wines of the country. For example, Picpoul is associated with the words apply, brisk, and kumquat.\n\n\n\nVariety Words\nFigure 2 presents the keywords associated with 13 grape varieties. Here again, we can see the top words are generally the varieties themselves, but the words that follow are, on the whole, adjectives used to describe the wine. Pinot Noir, for instance, is described using the words cherry, cola, and raspberries.\n\n\n\nFigure 2: Top keywords associated with the wines of 13 different varieties based on tf-idf values. In majority of cases, the first word in each sub-plot is the wine variety for that country. The words following are descriptors of the wine itself. For example, Rosé is described with the words cherry, strawberry, and raspberry.\n\n\n\nTopic Modelling\nPrior to analysing the data with topic modelling, we will reduce the data to a subset of countries, specifically: France, Italy, and Spain. These three countries were selected due to being the largest producers of wine in the world (in 2014). Table 1 presents the number of reviews associated with each of the countries selected for the topic model analysis. Whereas, Table 2 presents the count of varieties within the aforementioned three countries, specifically with a review count value exceeding 1000.\nEU County Counts\n\nTable 1: Number of reviews for each of the countries selected for the topic model analysis. It can be seen that Italian wines have a higher number of reviews, whilst Spanish wines have the fewest.\nCountry\nN\nSpain\n5370\nFrance\n10185\nItaly\n12155\n\nVariety Counts\n\nTable 2: Number of reviews exceeding 1000 for each of the wine varieties within the three countries (France, Italy, and Spain) chosen for the topic model analysis. It can be seen that Red Blend wines have a higher number of reviews followed by Chardonnay.\nVariety\nN\nTempranillo\n1348\nSangiovese\n1477\nRed Blend\n2909\nChardonnay\n1846\nPinot Noir\n1098\nBordeaux-style Red Blend\n1678\n\nTopic Extraction\nThe structural topic modelling package is used to analyse the reviews across the three countries (France, Italy, and Spain). In regards to model specification, we specify that the prevalence of a topic varies across country. Therefore, K-1 dummy variables are created with the baseline category being Italy (most frequent category). In addition to this, stemming was applied to the review data and the word ‘wine’ was dropped.\n\n\n\nAs there was information on the number of topics to extract, we follow a data-driven approach to identifying a suitable number of topics. Below we use the furrr package to run 10 topic models and extract the Exclusivity and Semantic Coherence values from each run. Exclusivity is a measure of how exclusive a word is to a topic; whereas, Semantic Coherence provides a measure of topic coherence (i.e., do the words forming a topic make sense semantically?). As shown in Figure 3 we can see these values (Exclusivity and Semantic Coherence) plotted against one another for models with 8, 10, 14, and 18 topics. If we look at the model with 18 topics, it can be seen that exclusivity is maximised yet semantic coherence becomes a lot lower for certain topics. An 8 topic solution, on the other hand, does show the exclusivity values to be quite spread out, whilst semantic coherence does not appear as problematic. A 10 topic model, does appear as a good balance between exclusivity and semantic coherence, and for this reason it was selected as the candidate model.\n\n\nk <- seq(2, 20, 2)\n\ntm_model_check <-\n  furrr::future_map(k, function(topic) {\n    tm_mod <-\n      stm(\n        documents = wine_reviews_prep$documents,\n        vocab = wine_reviews_prep$vocab,\n        K = topic, \n        prevalence = ~ country_France + country_Spain,\n        data = wine_reviews_prep$meta,\n        verbose = F\n      )\n    exclus <- exclusivity(tm_mod)\n    sem_coh <- semanticCoherence(tm_mod, wine_reviews_prep$documents)\n    mod_opt <- list(topic=topic, exclusivity=exclus, semantic_coherence=sem_coh)\n    return(mod_opt)\n  })\n\ntm_model_check <-\n  rbindlist(tm_model_check)\n\nggplot(tm_model_check[topic %in% c(8,10,14,18)], aes(x = exclusivity, y = semantic_coherence,\n                           colour = as.factor(topic),\n                           shape = as.factor(topic))) +\n  geom_point(\n    size = 2.5\n  ) +\n  labs(\n    x = \"Exclusivity\",\n    y = \"Semantic Coherence\",\n    colour = \"Topic\",\n    shape = \"Topic\"\n  ) +\n  scale_colour_manual(values =  wes_palette(\"Darjeeling1\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.key = element_rect(\n      colour = \"black\",\n      fill = \"white\"\n    ),\n    panel.background = element_rect(\n      colour = \"black\",\n      fill = \"white\"\n    )\n  )\n\n\n\n\nFigure 3: Ten LDA models were ran in increments of 2, starting from a 2 topic model up to a 20 topic model. For each model, the semantic coherence and exclusivity of the topics were extracted. A good balance between exclusitivty and semantic coherence seems to be found with a 10 topic model.\n\n\n\nModel Run\nFigure 4 presents the expected topic proportions (based on the document-topic matrix) along with the top five words associated with each topic (based on the word-topic matrix). To flesh out these topics, we will present two relevant quotes for each of the top 4 topics (1, 4, 7, and 9).\n\n\n\nFigure 4: Expected topic proportions across wine reviews along with the top 5 words per topic. Topics 1 and 4 appear to be found across a far greater number of wine reviews and appear to refer to the ageing and acidity of wines, respectively. Topic 3, on the other hand, appears to be far less frequent across wine reviews.\n\n\n\n\n\n\nTopic 1 – Acidity of Wine\nAs shown in Figure 4, the main words associated with Topic 1 are: acid, flavor, fresh, fruit, and fruiti. The quotes below both use the words fresh and acid to describe the wines, suggesting that Topic 1 is capturing wine acidity. According to this article, the acidity of a wine refers to the “fresh, tart, and sour attributes”.\n\n\n\nTopic 4 – Aging of Wine\nTopic 4 has the following top words associated with it: fruit, age, tannin, year, and rich (Figure 4). Both quotes that follow describe the wines in relation to the wine’s age, its intensity, and the level of tannins. From this, we can think of Topic 4 as capturing reviews towards those wines that have been aged.\n\n\n\nTopic 7 – Aroma of Wine (Red)\nTopic 7 is associated with the following words: spice, cheri, fruit, aroma, and soft (Figure 4). The two quotes associated with this topic show reviews that are centered upon the aroma of the wine, describing such things as the spice and tobacco. This topic can be discerned from Topic 9 as it appears that Topic 7 is specifically describing the aroma of red wines.\n\n\n\nTopic 9 – Aroma of Wine (White)\nTopic 9 is associated with the following words: aroma, white, fruit, fresh, and almond (Figure 4). From both quotes that follow, it can seen that the reviews are focused on the aromas of the wine, describing them in terms of citrus, cut grass, and white flower. As mentioned above, this topic is focused on the aromas of white wines, whilst Topic 7 was capturing reviews describing the aroma of red wines.\n\n\n\nCovariate Effects\nAs stated above, the topic model included covariates for topic prevalence; these covariates were dummy coded with Italy as the baseline. The plots that follow show the difference between the baseline (Italy) and each condition (France and Spain). To remain consistent with the aforementioned topic modelling results, the covariate effects with only be discussed in relation to Topics 1, 4, 7, and 9.\n\n\n\nFrance compared to Italy\nFigure 5 presents the difference in topic prevalence between France and Italy. For Topic 1 and 4, the prevalence in reviews was greater for wines from France. Topic 7 and 9 were less likely in French wine reviews compared to Italian wine reviews. This may suggest that French wines are fresh and tart, probably when it comes to white wines. Whereas, for red wines they are more likely to be aged and bold in flavour. Italian wines reviews, on the other hand, have a higher propensity of topics associated with the aromas of the wine (i.e., Topics 7 and 9).\n\n\n\nFigure 5: Topic prevalence differences between France and Italy (Italy is used as the baseline country).\n\n\n\nSpain compared to Italy\nFigure 6 shows that there are no meaningful differences for the topic prevalence of Topic 1 or 4 between Spain and Italy. As with French wine reviews, Spanish wine reviews are less likely to contain topics about the aromas of the wine.\n\n\n\nFigure 6: Topic prevalence differences between Spain and Italy (Italy is used as the baseline country).\n\n\n\nSummary\nUtilising different text-mining approaches, we have been able to explore a collection of wine reviews. Keyword extraction provided a succinct way of summarising wine reviews by both country and variety. Whereas, topic modelling provided greater depth in the exploration of wine reviews, particularly with the inclusion of covariates.\n\n\n\n",
    "preview": "posts/2021-02-18-wine-reviews/wine-reviews_files/figure-html5/countryTfidf-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-melbourne-covid-megathread-repost/",
    "title": "Melbourne Covid Megathread",
    "description": "This post details the analysis of posts made on the Melbourne Reddit covid megathread over the period of 7 months.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-02-18",
    "categories": [
      "reddit",
      "text mining"
    ],
    "contents": "\nOverview\nTo keep myself sane during lockdown, I decided to analyse public opinion towards the Victorian lockdowns, specifically those opinions vocalised on Reddit. Below you’ll find various pieces of analysis of megathread posts covering the period from ‘2020-03-03’ to ‘2020-09-13’.\nPosting Behaviour\n\n\n\nFigure 1 shows the daily count of posts during the time frame (2020-03-03 to 2020-09-13), along with the 14 day average (in red). Key events in the Covid-19 timeline for Victoria have been marked with the view of providing some context to the posting behaviour, specifically those events during the second wave. The main takeaway from the Figure 1 is how the number of posts has followed the waves, with relatively low post numbers between March and April, and a uptake in posting that began to steadily rise towards the end of June.\n\n\n\nFigure 1: Daily number of posts in the Covid-19 megathread over the period of 2020-03-03 to 2020-09-13. The white line shows the raw daily count of posts, whilst the red line is the 14 day average. It can be seen that posting on the megathread was particularly low during the first wave, yet surged during Victoria’s second wave. Key dates during the second wave have been highlighted to provide context for posting behaviour.\n\n\n\nPosting During the First Wave\nCases of Coronavirus began to rise in Victoria during March, resulting in Daniel Andrews (Premier of Victoria) declaring a State of Emergency on the 16th of that month. This also corresponded to the time when panic buying was all the rage, and daily excursions to locate toilet paper were not uncommon. In the realm of the Melbourne sub-Reddit, posts associated with Covid-19 were also on the rise (Median # of posts per day in March = 508; median absolute deviation = 333.58). The median absolute deviation shows that, on average, the number of day-to-day postings during March were 333.58 posts away from the median.\nThere is quite a lot of spread in the number of posts made during the month of March, with some days being particularly low (e.g., 75 posts on 2020-03-05), whilst some were relatively high (e.g., 1136 posts on 2020-03-17). Interestingly, these 1136 posts fell on the day after the state of emergency was declared. Table 1 presents a random sample of 5 posts from 2020-03-17. The topic of ‘panic buying’ is quite clear from this sample of messages, with some posters referencing supermarkets running out of toilet paper, whilst others appear to have prepped for this situation.\n\n\n\n\nTable 1: A random sample of 5 posts taken from 2020-03-17, corresponding to the date following the State of Emergency being declared in Victoria. General themes in these posts include the lack of toilet paper in supermarkets, prepping for the supposed apocalypse, and following social distancing guidelines.\nCreated Date\nPost Text\n2020-03-17\nWe’re being made to go into work, despite a confirmed case in our building :-( Take whatever steps you can to keep yourself safe. I hope they change the mandatory attendance requirements for your class soon\n2020-03-17\nSpend some time in r/conspiracy\n2020-03-17\nYeah, I ain’t gonna be hugging randoms anytime soon, but I’m certainly getting out on the bike with these quieter roads!\n2020-03-17\nMaybe just be prepared in the future? Why is panic buying or having nothing the only 2 options? At the start of the year i made sure to buy an extra 24 pack of toilet paper and a months food for my partner and myself, as well as cleaning supplies, soap, board games etc As a result we are not worried about food supplies or toilet paper as we prepared in advance and we are not taking away supplies from poor or elderly people. Dont blame your lack of foresight on others, you control your life\n2020-03-17\nJust did a walk through a lot of the CBD (I live here), no TP anywhere.\n\nPosting During the Second Wave\nDespite lockdown relaxations around the beginning of June, these were short-lived. By the 20th June various restrictions came into force, with postcode lockdowns being enforced on the 30th June. During the month of June we can see a stark contrast in posting behaviour. If we take the period from the 2020-06-01 to 2020-06-19 (inclusive), the median number of posts were 44 (median absolute deviation = 25.2042). Compare this to period between 2020-06-20 to 2020-06-30, wherein the median number of posts rose to 294 (median absolute deviation = 139.3644), representing a 7-fold increase in the average day-to-day posting.\nReturning to Figure 1, there are three discernible peaks in posting behaviour over the period of July to September. These peaks align with various lockdown announcements. On 2020-07-07, when Stage 3 Stay at Home restrictions were announced, there were 4304 posts made. 2020-08-02 was when the State of Disaster was declared and the move to Stage 4 restrictions; the number of posts made on this day was 7295. Finally, on 2020-09-06 the Roadmap was unveiled, resulting in another uptick in postings (8277).\nTable 2 contains a random sample of 15 sentences from the aforementioned dates (i.e., ‘2020-07-07’, ‘2020-08-02’, and ‘2020-09-06’). The sentences were first obtained by concatenating the posts for each date and applying the Gensim summarizer function to summarise the messages. For the ‘2020-07-07’, messages seem to refer to questions around leaving metro Melbourne for regional areas and the concern around spending 6 weeks in lockdown. Messages on ‘2020-08-02’ seem to be centred on seeking clarification for the various Stage 4 restrictions and vocalising their feelings of apathy towards a pro-longed lockdown. One message in the sample is associated with ‘2020-09-06’ and succinctly summarises how we should not get into the habit of focusing on daily case announcements.\n\n\n\n\nTable 2: Sample of 15 sentences across the dates of ‘2020-07-07’, ‘2020-08-02’, and ‘2020-09-06’. These sentences were obtained by applying the gensim summarizer function to a concatenated list of posts for each of the aforementioned dates. In this way, there is a likelihood that two or more sentences could originate from a single post.\nCreated Date\nPost\n2020-07-07\nI’m renting in Melbourne but my family lives in regional Vic. Any idea if I could move home in (for example) a few weeks if everything goes to shit?\n2020-07-07\nThat’s fair enough but it still happened here while we had active community cases, and the stuff I saw from friends on social media wasn’t in line with social distancing practices and is likely still contributing, we still have a whole bunch of untraced cases and people heeding my advice and being more conscious of the impact of behaving that way is worth people considering regardless of the other factors that lead to this.\n2020-08-02\nwhy do we listen to all these elected leaders, that are informed by teams of professionals that are considered experts in their fields, when we have people like you around?\n2020-07-07\nIt’s worth keeping in mind that this is a very infectious and very nasty virus, and for a city of nearly 5m people, we’re overall doing a pretty good job of keeping it under control.\n2020-07-07\nCovids are thinking exactly the same No we didn’t, clearly the virus was still out there last time when people were supposed to be self-isolating, and it ain’t leaving.\n2020-08-02\nIf your business dosnt facilitate essential supplies or pharmacist / providing care, I imagine your business will be either work from home, or flat out closed.\n2020-08-02\nFor those interested, here’s today (Sunday August 2)’s press conference: https://www.youtube.com/watch?v=rkYfnfYwO8o pretty much sums up this week…\n2020-07-07\nYour government is about a week behind where they need to be.\n2020-08-02\nit worked in every other country No, see Dan is secretly hoping people will start killing themselves off and going into deep depression because his ultimate goal is to watch the state crumble and burn…\n2020-08-02\nHopefully they don’t ban movers though, I am married to a ‘collector’ and have a lot of things They specifically say you’re allowed 5ks from the location you sleep, if you’re moving then that’s fine.\n2020-08-02\nI completely agree and someone who has lived almost their entire life and whats more as part of a privileged generation should not be the determining factor in destroying our nations economy and the mental health of people with their whole lives ahead of them.\n2020-07-07\nI also understand six weeks might feel like an eternity.\n2020-09-06\nWe absolutely can’t get into a world of \"“we need less than X cases per day before we open”\"…\n2020-08-02\nCan only think it’s to prevent parties/gatherings and people avoiding police by travelling at night.\n2020-08-02\nPushing the blame to people after hotel quarantine, contact tracing and age care fail AND when most contaminations occurred in work places.\n\nSentiment Analysis\nFigure 2 presents the day-to-day distribution of the Vader compound scores, facetted by month. The use of Vader to measure sentiment is applicable in this context as it is suited to social media data. The compound score corresponds to a normalisation of the summed valence scores across words within the message. As can be seen in Figure 2, compound scores appeared to be quite variable in the months of April, May, and June. Given that the number of posts during these months were relatively low, these distributions are not unexpected. During those months wherein posting behaviour was higher (March, July, August, and September), we can see that the day-to-day compound score had a peak at 0. It may be the case that posts were generally neutral, outweighing those that were overly positive or negative in sentiment. We also have to consider the confounding factor that messages will have been moderated to an extent; therefore, we may not be seeing a complete representation of postings.\n\n\n\nFigure 2: Day-to-day distribution of Vader Compound Scores for each post across each month. Plots show that there was a high variability in compound scores during April, May, and June, which is a likely a result of there being a smaller number of posts during these months. Months such as July and August show the distributions to peak at 0, a likely result of there being a far greater number of posts during this time.\n\n\n\nSummary\nAlthough not representative of the larger population, posting behaviour within the megathread has offered some interesting insights. For one, it can be seen that posting has seemingly followed the number of reported infections. As this data only reflects up to ‘2020-09-13’ this conclusion is, however, limited. In conjunction with the information of the message content, we can also see how topics within the posts have shifted over the course of these 7 months.\nTo end this post, I present Figure 3 that presents keywords (using the Gensim keywords function) from those messages with a Vader compound score exceeding .4 (i.e., they have a positive sentiment).\n\n<wordcloud.wordcloud.WordCloud object at 0x7f9e12447490>\n(-0.5, 755.5, 979.5, -0.5)\n\n\nFigure 3: Wordcloud of those keywords in messages with Vader compound scores exceeding the value of .4\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-18-melbourne-covid-megathread-repost/melbourne-covid-megathread-repost_files/figure-html5/postplot-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-06-frasier-analysis/",
    "title": "Frasier Analysis",
    "description": "In this post I undertake a descriptive and text mining analysis of the lines spoken on Frasier.",
    "author": [
      {
        "name": "Alex Wainwright",
        "url": "https://github.com/alexwhitelockw"
      }
    ],
    "date": "2021-02-06",
    "categories": [
      "frasier",
      "text mining"
    ],
    "contents": "\nOverview\nAfter re-watching Frasier a few times over the course of the Victoria lockdowns, I decided to undertake a project to extract and analyse text contained in the Frasier scripts. After a weekend spent getting to grips with the BeautifulSoup and Request libraries, I have managed to extract and clean the data (not 100% guaranteed). The original data source can be found here and can be downloaded here.\n\n\n\nCharacter Lines\nBy Season\nLet’s first start by exploring some descriptives assoicated with the number of lines each character has across the 11 seasons. Focus of this analysis is solely on the five main characters: Daphne, Frasier, Martin, Niles, and Roz. Table 1 and Figure 1 show the average number of lines per character across seasons. We can see that Frasier obviously dominates in the number of lines spoken and, for the most part, varying between 80 to 90 lines per season. However, seasons 9 and 10 show a dip to 70 and 67 lines, respectively. It may have been the case that episodes across these 2 seasons emphasised other characters (e.g., Roz’s new job and Nile’s heart surgery take place in season 10). Nile’s lines peaked in season 6, which is understandable as his story centers on his divorce from Maris.\n\nTable 1: The average (mean) number of lines for each of the five main characters on Frasier across 11 seasons. As expected, Frasier consistently has the highest number of lines of any of the characters, followed by Niles and Martin. Daphne and Roz appear to be comparable, but have far fewer lines over the seasons than Frasier, Martin, or Niles.\nCharacter\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nDaphne\n18.75\n16.25\n17.92\n22.21\n24.75\n18.21\n23.54\n27.89\n24.12\n24.12\n18.70\nFrasier\n91.50\n84.21\n82.21\n81.04\n85.33\n92.75\n83.92\n80.33\n70.08\n66.67\n90.54\nMartin\n34.04\n28.29\n29.04\n30.38\n27.17\n33.00\n32.25\n25.75\n31.08\n30.29\n28.67\nNiles\n33.42\n35.25\n35.92\n43.79\n42.21\n52.46\n39.75\n44.29\n39.62\n37.21\n40.25\nRoz\n13.04\n18.25\n15.62\n16.92\n21.62\n23.08\n21.42\n20.33\n19.12\n23.71\n16.73\n\nMartin is quite consistent across the 11 seasons in regards to the average number of lines. The same can generally be said of Daphne and Roz. Interestingly, Roz did see an uptick in lines beyond season 1.\n\n\n\nFigure 1: Average (mean) number of lines spoken by the five main characters in Frasier across 11 seasons.\n\n\n\nBy Episode\nFigure 2 presents the number of lines by each of the main characters at the level of each episode. From this perspective, we can see far more clearly as to why Frasier seemingly showed, on average, fewer lines during seasons 9 and 10. Additionally, you can see occasions when the number of lines spoken by Niles exceeds those of Frasier. For example, Kelsey Grammer directed Season 3 Episode 13 (Moon Dance), resulting in the character of Frasier making a very limited appearance. Instead, the episode largely focused on the dynamics between Niles and Daphne, hence the greater number of lines for these two characters. We can also see another occasion in Season 6 Episode 16 (Decoys) wherein Niles devises a plan to reunite Roz and her ex-boyfriend Donny.\n\n\n\nFigure 2: Number of lines spoken by the five main characters in Frasier across each episode for the 11 season run.\n\n\n\nKeywords\nBeyond an examination of the number of lines spoken by each of the main characters, we can also explore the keywords associated with each episode. To do this, we can use the Term Frequency - Inverse Document Frequency measure (tf-idf). This statistic is essentially the frequency of words multiplied by how common those words are across all documents (i.e., words that are found across many documents are downweighted, whilst rare words are assigned a higher weight). Given that there are 264 episodes of Frasier, exploring the keywords each episode would be tedious. Instead, a random sample of 5 episodes (Figure 3) is used and we will explore how well the keywords summarise each episode.\n\n\n\nFigure 3: Top words, based on tf-idf, across a random sample of 5 episodes.\n\n\n\nAnd the Dish Ran Away with the Spoon\nThis is a two part episode (Season 8 Episode 1 and 2) where Daphne and Niles confess their love to Donny and Mel. The associated keywords do well to summarise the episodes: the story centers on Daphne and Donny’s wedding and the aftermath of the aforementioned confession (e.g., Donny suing Daphne and Frasier).\nDial M for Martin\nThis is episode 3 of season 6, wherein Martin moves in with Niles to give Frasier space. Words such as Rebecca, model, and lingerie refer to Frasier’s date and her occupation. Stairs is mentioned a few times:\nin relation to Martin being physically able to walk up and down the stairs at Nile’s place;\nin reference to Niles pushing Martin down the stairs as a way to keep Daphne from leaving;\nand, in the end when Martin exclaims that he cannot walk up and down the stairs at Nile’s place.\nMy Coffee with Niles\nThis is episode 24 of season 1, which revolves around Frasier and Niles sitting in Cafe Nervosa mulling over his (Frasier’s) move to Seattle and whether he is happy. The words do well in summarising the plot as throughout the episode Frasier continually returns coffee for various reasons (not decaf, no non-fat milk). Bumbershoot is the term Daphne uses to describe an umbrella.\nMy Fair Frasier\nThis is episode 7 of season 5 where Frasier dates an attorney (Sam Pierce). Although only 4 keywords are presented, they do give the gist of the episode. In effect, Frasier is concerned that his new relationship, which started over the purchase of a purse, puts him in a position where he is not dominant.\nThe Ann who Came to Dinner\nFinally, we have episode 13 of season 11. As a result of Frasier’s fearing being sued (due to having no insurance), Ann moves in with him an Martin. We can also see the words trumpet, which is what Ann plays, and bunny, the nickname Ann uses for Frasier.\nConclusion\nAlthough brief, the analysis has shown how the number of lines per character changed over the 11 seasons. Moreover, the use of tf-idf has been helpful in identifying keywords that can do well in summarising episodes.\n\n\n\n",
    "preview": "posts/2021-02-06-frasier-analysis/frasier-analysis_files/figure-html5/linesPlot-1.png",
    "last_modified": "2021-12-19T16:24:21+11:00",
    "input_file": {}
  }
]
